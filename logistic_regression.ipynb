{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolframalexa/FrequentistML/blob/master/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pokc_8cOt1I3",
        "colab_type": "text"
      },
      "source": [
        "Source: [UCI ML Repository Breast Cancer Dataset](https://https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCEwaJwjnC2t",
        "colab_type": "text"
      },
      "source": [
        "This dataset contains information about skin masses and whether they are cancerous. The goal is to apply logistic regression to determine whether a mass of cells is cancerous or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WrW6NQDsVwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------IMPORT PACKAGES\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krq5rUZquIF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------DATA READING \n",
        "\n",
        "data = 'https://raw.githubusercontent.com/wolframalexa/FrequentistML/master/datasets/breast-cancer-wisconsin.csv'\n",
        "col_names=['ID','clump_thickness','cell_sz_unif','cell_shape_unif','marginal_adhesion','epi_cell_sz','bare_nuclei','bland_chromatin','normal_nucleoli','mitoses','class']\n",
        "dataframe = pd.read_table(data, sep=',',header=None,names=col_names,na_values='?')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOnLXeVOxLFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "4354f676-1f1c-47b3-f86c-9e8c584d2f21"
      },
      "source": [
        "#----------CLEAN DATA\n",
        "\n",
        "# Add column to account for intercept \n",
        "ones_col = np.ones(len(dataframe))\n",
        "dataframe.insert(0, \"intercept\", ones_col, True) \n",
        "\n",
        "# Drop rows with missing data\n",
        "clean_data = dataframe.dropna(axis=0)\n",
        "\n",
        "# Adjust class - 0 is benign, 1 is malignant\n",
        "clean_data['class'] = (clean_data['class']-2)/2\n",
        "\n",
        "# Show data information                    \n",
        "clean_data.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intercept</th>\n",
              "      <th>ID</th>\n",
              "      <th>clump_thickness</th>\n",
              "      <th>cell_sz_unif</th>\n",
              "      <th>cell_shape_unif</th>\n",
              "      <th>marginal_adhesion</th>\n",
              "      <th>epi_cell_sz</th>\n",
              "      <th>bare_nuclei</th>\n",
              "      <th>bland_chromatin</th>\n",
              "      <th>normal_nucleoli</th>\n",
              "      <th>mitoses</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>683.0</td>\n",
              "      <td>6.830000e+02</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "      <td>683.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.076720e+06</td>\n",
              "      <td>4.442167</td>\n",
              "      <td>3.150805</td>\n",
              "      <td>3.215227</td>\n",
              "      <td>2.830161</td>\n",
              "      <td>3.234261</td>\n",
              "      <td>3.544656</td>\n",
              "      <td>3.445095</td>\n",
              "      <td>2.869693</td>\n",
              "      <td>1.603221</td>\n",
              "      <td>0.349927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>6.206440e+05</td>\n",
              "      <td>2.820761</td>\n",
              "      <td>3.065145</td>\n",
              "      <td>2.988581</td>\n",
              "      <td>2.864562</td>\n",
              "      <td>2.223085</td>\n",
              "      <td>3.643857</td>\n",
              "      <td>2.449697</td>\n",
              "      <td>3.052666</td>\n",
              "      <td>1.732674</td>\n",
              "      <td>0.477296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>6.337500e+04</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>8.776170e+05</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.171795e+06</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.238705e+06</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.345435e+07</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       intercept            ID  ...     mitoses       class\n",
              "count      683.0  6.830000e+02  ...  683.000000  683.000000\n",
              "mean         1.0  1.076720e+06  ...    1.603221    0.349927\n",
              "std          0.0  6.206440e+05  ...    1.732674    0.477296\n",
              "min          1.0  6.337500e+04  ...    1.000000    0.000000\n",
              "25%          1.0  8.776170e+05  ...    1.000000    0.000000\n",
              "50%          1.0  1.171795e+06  ...    1.000000    0.000000\n",
              "75%          1.0  1.238705e+06  ...    1.000000    1.000000\n",
              "max          1.0  1.345435e+07  ...   10.000000    1.000000\n",
              "\n",
              "[8 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WFoZdcjzPVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------SEPARATE DATA\n",
        "\n",
        "# Choose seed\n",
        "np.random.seed(420)\n",
        "\n",
        "# Separate training and testing data\n",
        "train_data, validate_data, test_data = np.split(clean_data.sample(frac=1), [int(.8*len(clean_data)), int(.9*len(clean_data))])\n",
        "\n",
        "# Separate training inputs and outputs\n",
        "x_train = train_data.drop(['class','ID'], axis=1)  \n",
        "y_train = train_data['class']\n",
        "\n",
        "# Separate validation inputs and outputs\n",
        "x_val = validate_data.drop(['class','ID'], axis=1)  \n",
        "y_val = validate_data['class']\n",
        "\n",
        "# Separate testing inputs and outputs\n",
        "x_test = test_data.drop(['class','ID'], axis=1)\n",
        "y_test = test_data['class']\n",
        "\n",
        "\n",
        "# Normalize testing validation and training inputs\n",
        "x_train = (x_train-x_train.min())/(x_train.max()-x_train.min()) \n",
        "x_train.intercept = np.ones(len(x_train))\n",
        "x_val = (x_val-x_val.min())/(x_val.max()-x_val.min()) \n",
        "x_val.intercept = np.ones(len(x_val))\n",
        "x_test = (x_test-x_test.min())/(x_test.max()-x_test.min()) \n",
        "x_test.intercept = np.ones(len(x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e28BmOVT5KZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "84c5b089-94c3-4e9f-bced-037b319a8f2b"
      },
      "source": [
        "#------------LOGISTIC REGRESSION WITH STOCHASTIC GRADIENT DESCENT\n",
        "# set initial thetas\n",
        "thetas = np.ones(len(x_train.columns))\n",
        "\n",
        "# for plotting later\n",
        "likelihood_no_reg = np.ones([1,1000])[0]\n",
        "\n",
        "# use stochastic gradient descent\n",
        "for i in range(1000):\n",
        "  alpha=0.01 # step size\n",
        "  h = 1/(1 + np.exp(-np.dot(x_train,np.transpose(thetas))))\n",
        "  error = y_train - h\n",
        "  thetas = thetas + alpha * np.dot(error,x_train)\n",
        "\n",
        "  # for plotting later, calculate likelihood function\n",
        "  likelihood_no_reg[i] = np.multiply(h**y_train,(np.ones(h.shape)-h)**(1-y_train))[0]\n",
        "\n",
        "print(\"Logistic regression coefficients:\",thetas)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic regression coefficients: [-7.36995746  5.06161463  0.36119204  2.62576041  2.41971287  0.60631737\n",
            "  3.56808679  4.81282288  2.52051775  3.22786641]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G8cmMQ0dBVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1865ccf6-ffee-44d1-ef13-7b3f24bd7ff2"
      },
      "source": [
        "# score on test set\n",
        "benchmark = 0.5 # set decision boundary\n",
        "\n",
        "# apply thetas \n",
        "logistic_sga_predicted = 1/(1 + np.exp(-np.dot(x_test,np.transpose(thetas))))\n",
        "logistic_sga_predicted_class = [0 if i<=benchmark else 1 for i in logistic_sga_predicted]\n",
        "\n",
        "# XOR lists together to find where classes are different, and divide by length to get % incorrect\n",
        "pred_incorrect = sum(list(map(bool,logistic_sga_predicted_class)) ^ y_test.astype('bool'))/len(logistic_sga_predicted_class) \n",
        "print(\"Percent of incorrect predictions:\",pred_incorrect*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent of incorrect predictions: 2.898550724637681 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApQs6yezmRNX",
        "colab_type": "text"
      },
      "source": [
        "The decision boundary was set at 0.5, meaning that if there was more than a 50% chance that the cells were malignant, they were marked as such, otherwise, they were marked as benign. Of course, this number may change in the real world (it may make more sense to tell people they have cancer when you are more sure, or if you are aggressive, you may want to order further tests even if there is a smaller chance it is cancer). The model, using logistic regression, has a 2% error rate on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNT_LgiN6mDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "9383494f-d3d0-4f38-82c0-a15c88124606"
      },
      "source": [
        "#------------LOGISTIC REGRESSION WITH SGD AND REGULARIZATION\n",
        "# set initial thetas\n",
        "thetas_reg = np.ones((1000, len(x_train.columns))) # one row for each lambda\n",
        "\n",
        "# set vector of lambdas\n",
        "lambdas = np.linspace(0, 0.1, num=10) # I want to test more smaller values than larger values\n",
        "\n",
        "# initialize scoring vector\n",
        "vals = np.zeros(lambdas.shape)\n",
        "\n",
        "# set likelihood vector\n",
        "likelihood_reg = np.ones((1,1000))[0]\n",
        "\n",
        "# use stochastic gradient descent for each lambda\n",
        "alpha=0.01 # step size\n",
        "benchmark = 0.5\n",
        "\n",
        "for index,lam in enumerate(lambdas):\n",
        "  for i in range(1000):\n",
        "    h = 1/(1 + np.exp(-np.dot(x_train,np.transpose(thetas_reg[index]))))\n",
        "    error = y_train - h\n",
        "\n",
        "    # update thetas using SGD and L2 regularization penalty\n",
        "    thetas_reg[index] = thetas_reg[index] + alpha * np.dot(error,x_train) - lam * 2 * thetas_reg[index]\n",
        "\n",
        "    # for plotting later, calculate likelihood function\n",
        "    likelihood_reg[i] = np.multiply(h**y_train,(np.ones(h.shape)-h)**(1-y_train))[0]\n",
        "\n",
        "  log_sgd_reg_pred = 1/(1 + np.exp(-np.dot(x_val,np.transpose(thetas_reg[index]))))\n",
        "  log_sgd_reg_pred_class = [0 if i<=benchmark else 1 for i in log_sgd_reg_pred]\n",
        "\n",
        "  # XOR lists together to find where classes are different, and divide by length to get % incorrect\n",
        "  pred_incorrect = sum(list(map(bool,log_sgd_reg_pred_class)) ^ y_val.astype('bool'))/len(log_sgd_reg_pred_class) \n",
        "  vals[index] = pred_incorrect\n",
        "\n",
        "# Find lambda that minimizes error on val set\n",
        "minpos = np.where(vals == vals.min())\n",
        "minpos = minpos[0][0] # choose first one if many local minima\n",
        "best_lam = lambdas[minpos]\n",
        "\n",
        "print(\"Best lambda chosen on validation set:\",best_lam)\n",
        "thetas_reg_best = thetas_reg[minpos]\n",
        "print(\"Best coefficients chosen on validation set:\",thetas_reg_best)\n",
        "\n",
        "# Apply new thetas to test set and score\n",
        "log_sgd_reg_pred = 1/(1 + np.exp(-np.dot(x_test,np.transpose(thetas_reg_best))))\n",
        "log_sgd_reg_pred_class = [0 if i<=benchmark else 1 for i in log_sgd_reg_pred]\n",
        "\n",
        "# XOR lists together to find where classes are different, and divide by length to get % incorrect\n",
        "pred_incorrect_sgd = sum(list(map(bool,log_sgd_reg_pred_class)) ^ y_test.astype('bool'))/len(log_sgd_reg_pred_class) \n",
        "print(\"Percent of incorrect predictions:\",pred_incorrect_sgd*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best lambda chosen on validation set: 0.0\n",
            "Best coefficients chosen on validation set: [-7.36995746  5.06161463  0.36119204  2.62576041  2.41971287  0.60631737\n",
            "  3.56808679  4.81282288  2.52051775  3.22786641]\n",
            "Percent of incorrect predictions: 2.898550724637681 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVmVzPynqKJa",
        "colab_type": "text"
      },
      "source": [
        "Using stochastic gradient descent, we chose the best $\\lambda$ value by minimizing the cost function. In the case where multiple values minimized the function, we chose the first. With an error rate of approximately 3%, only two cases are misidentified, which is the same percentage as logistic regression without SGD. Because the test dataset only has 69 entries, this percentage could certainly change with a larger dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXVrTO6mutCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "832a7edc-3f25-4256-b113-59ea324f0e1b"
      },
      "source": [
        "#-------PLOT LIKELIHOOD FUNCTION\n",
        "\n",
        "# prepare iterations\n",
        "i_graph = np.linspace(1,1000,num=1000)\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "likelihood_reg = np.transpose(likelihood_reg)\n",
        "likelihood_no_reg = np.transpose(likelihood_no_reg)\n",
        "\n",
        "plt.plot(i_graph,likelihood_reg)\n",
        "plt.plot(i_graph,likelihood_no_reg)\n",
        "\n",
        "plt.title('Likelihood Function as a Function of Iterations of SGD')\n",
        "plt.ylabel('Likelihood function')\n",
        "plt.xlabel('Number of iterations')\n",
        "plt.legend(['With regularization', 'Without regularization'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgU1fX/8fdhWAYEQUBRNjGJG0E2ERfEXaLGuESNoiYqKkaDuz9DNsUlCe4JavLVRMVE4xJNDBKNigoJguKgKLIIiMgiKqAgoizDnN8fdXuoHnt6eoap6Znpz+t5+umupatOVXXX6Xtv9S1zd0REpHA1yXcAIiKSX0oEIiIFTolARKTAKRGIiBQ4JQIRkQKnRCAiUuAKJhGY2WAzezc2vMjMjqjBcsrfZ2Y/N7M/h9c9zMzNrGntRV1pDBPN7Lyk11NTZjbLzA7JdxyNhZl9YWbfqON1tjSzp81sjZn9vS7XnSsz+z8z+1W+40gxs93NbIaZrTWzS/IdT3U0ukRQ2Qne3f/n7rvX5rrc/TfuXq9OyGY2ysw2hZNH6nF1gusba2Y3xse5+7fdfWJS66xLYfs2Vtifpya4vq8leXdv7e4Lk1pnJU4GOgEd3P2UihPD5+yh2LCb2beSCsbMzjazyfFx7v5jd78hqXXWwNXAy+7ext3HVJxoZt82s+fN7FMzW21m083smNj0NmZ2eziHrTOzxWb2hJntG5vHw7QvzGyVmb1YG5/HRpcIBIDHwskj9bg53wE1cDdX2J+P5TugOrAzMM/dS5NeUV2UouvIzsCsLNOfBl4AdgR2AC4BPgcwsxbAS8BewLHAtsCewKPA0RWW08fdWwO7A2OBu8zs2q2K3N0b1QNYBByRYfwhwNJM84Ud/j4wNAwfC8wAVgNTgN6VvG8U8FB43QNw4CxgMbAS+EXsfS2A3wEfhsfvgBax6ecDC4BPgXFA59i0I4G5wBrgLmAScF4l218eU7bxsXibhuGJwA3AK8Ba4HmgY2z+A8O+WA0sAc4GhgObgI3AF8DTGfZRpdudOibAlcAnwHLgnCzH9hxgTohvIXBBbFpHYHyI71Pgf0CTSpbz+7ANnwPTgcFZ1jkWuLGq8WT+fF0FvB2O22NAcWz68USfsc+B94CjgF8Dm4H1YX/eFeZ14FvhdVvgL8AK4APgl6ntDMdkMnAr8BnRZ/roLNu2Zzjuq4lOYMeF8deFY7opxHFuts8T8N8Q47ow/6k5fo9+GvbPBqApMDLsi7XAbODEWJzrw775AlhdyTHI9h1y4MfA/BDP3YCFad8i+k6tIfrePpZlnx0X9tXqsO/2DONfqnDsdqvwvo4hhnaVLPc8os//NlWc38o/C7FxJ4f1dqjxebOmb6yvD6qZCID+RCfuY8P4fkQnpX2BIqIT+yK2nLzKl0/mRPAnoCXQJ3zAUx+U64FXiX4JbB++GDeEaYeFD2B/ohPnncB/Yx+gteFgNwMuB0pJJhG8B+wW4p8IjA7Tdg4xDA0xdAD6ZvoyZthH2bb7kLAt14flHgN8CWxXybZ9F/gmYMDBYd7+Ydpvgf8Ly2kGDCZ80TMs58ywDU2JktBHxE7SFeb92vZlGk/mz9c0oDPQniiB/ThMG0h00jmSqFTeBdgjdhzOq7CueCL4C/AvoE04hvMIJ2qiRLCJ6IRYBFxIlHy/th/CPloA/BxoTvQZXAvsnu1zlOXzlHaCIrfv0QygG9AyjDsl7K8mwKlEiWWn2LZNruwYkOU7FItvPNAO6E6USI8K0x4BfhHWWwwcWMk27xZiOjLsv6vDPmxe2bGLvdeIktB44ASgU4XpjwJjczi/ZUoEzYi+R5Um/aoehV41NJjol8OP3H18GDccuMfdX3P3ze7+INEJfb8cl3mdu3/l7m8BbxElBIAzgOvd/RN3X0H0q+uHsWn3u/sb7r4B+Bmwv5n1IDo5znL3J9x9E9Ev6o+qiOEHoQ4y9eicY+wPuPs8d/8KeBzoG8afDkxw90fcfZO7r3L3GTkuM9t2Q3Tiuj4s9xmiX1MZ23Lc/d/u/p5HJhGVWgbHlrMTsHNY1v88fEsyLOehsA2l7n4b0YkjW/vRVbF9uTLH7QYY4+4fuvunRNUCqf15LtHxfsHdy9x9mbvPrWphZlYEnAb8zN3Xuvsi4DbS9+cH7v4nd98MPEi0TzplWNx+QGuiZL/R3V8iOkkNrcb2ZZPL92iMuy8Jnzfc/e9hf5V5VP02nyhp5iLbdyhltLuvdvfFwMtsOR6biH7sdHb39e6e1hYRcyrw73DcNhGVvFoCB1QVXPgsHkqUAG8DlpvZf81s1zBLR2LfazPrGz5vn8cvcqlk2ZuIkmD7quKoTKEngh8DUzy9YXNn4Mr4iZToV0uuJ9P4SfpLoi8b4f0fxKZ9EFtm2jR3/wJYRfRLsTNRNUZqmseHK/G4u7eLPT7cyti7EZUWaiLbdgOs8vR66Ph605jZ0Wb2aqqxjShJdgyTbyH6dfa8mS00s5GVBWRmV5nZnHBFzGqi6paOlc0P3Brbl9nmq6i292dHol9/Ffdnl0zrdPcvw8tM+7MzsMTdy7Isa2vk8j1K+xyb2Y/CVTep+XuR/bjEZfsOpVR2PK4m+sU+LVzxNizHdZSFbchpn7n7Uncf4e7fJNo/64hKeIRYd4rNO8Pd2wHfJ/qhUikza0ZU2v40lzgyUSKA7mZ2R2zcEuDXFU6krdz9ka1c14dEBz+lexj3tWlmtg1R1cUyonrDbrFpFh+uhnVAq9jwjtV47xKiKplMMv7qjsm23TkLjWlPEv0K6xS+JM8QfYEJv5CvdPdvENXjXmFmh2dYzmCiL/4PiKqg2hFV01g1Q8rH/lzJll+vKd2JPifV9SHQzczi54CaLiuTXL5H5dtqZjsTVauOIKrrbge8w5bjUq3PWYXvUFbu/pG7n+/unYELgD9UcgVUxXWkvovV3mfuvoSonaJXGPUiMCTEXV3HE1UNTavBe4HGmwiamVlx7FHZVQlriRrpDjKz0WHcn4Afm9m+FtnGzL5rZm22MqZHgF+a2fZm1hG4BngoNu2cUBxsAfwGeC0U/f8NfNvMvh+24xKqd9JJmUG0nd3NrC1R0TlXDwNHmNkPzKypmXUws1Sx+mMg2zXu2ba7OpoT/TJaAZSa2dHAkNREMzvWzL4VvpxriBruyjIspw3Rl2YF0NTMriG6QqO6ZgDHmFl7M9sRuKwa772P6HgfbmZNzKyLme0RplW6P0N1z+PAr8OlhjsDV1Cz/fka0a/iq82smUX/+/geUV11TVSMu7rfo22ITvYrAMzsHLacJFPL72pmzSt5f7bvUFZmdoqZdQ2Dn4U4Mn12Hge+G45bM6L2pQ1E7V5VrWM7M7sufEabhO/CMKL2M4hKBsuBf5pZLzMrMrNiYECWZbY3szOIEspN7r6qqjgq01gTwTPAV7HHqMpmdPfVRI0/R5vZDe5eQtTYdhfRh2IBUUPV1roRKCG6SmIm8EYYh7tPAH5F9It3OdGvxdPCtJVEjWijiYqPuxJd2VMt7v4C0ZUrbxNdKTM++zvS3ruYqBrmSqLi5wy2tH3cB/QMxfmnMry90u2uZvxriZLg40TH5XSi9p2UXYEJRG0MU4E/uPvLGRb1HPAfokbWD4iutqiqqi2TvxK1AS0iaqvI+ZJSd59GdAXUHURJaxJbfmn+HjjZzD4zs69diw5cTFQaWUh0hdDfgPurG7y7byQ68R9NVNL4A1FbWZVtFZUYBTwYPgc/qO73yN1nE9WdTyU66e9F+uf8JaKrdT7K1E6T7TuUg32A18zsC6LP1KWe4X8b7v4u0YUGdxLts+8B3wv7siobiRr3JxBdKfYOURI5Oyx7PVEbwmyiH3+fA++G2H5QYVlvhVgXEF1tdLm7X5PjtmaUunxKREQKVGMtEYiISI6UCERECpwSgYhIgVMiEBEpcA2us6eOHTt6jx498h2GiEiDMn369JXuvn2maQ0uEfTo0YOSkpJ8hyEi0qCY2QeVTVPVkIhIgUssEZjZ/Wb2iZm9U8l0M7MxZrbAzN42s/5JxSIiIpVLskQwlqj7hsocTfRv0F2Jeir8Y4KxiIhIJRJLBO7+X7L3hnc88JfQpfCrQDsz2ynL/CIikoB8thF0Ib2Pl6VU0p2rmQ03sxIzK1mxYkWdBCciUigaRGOxu9/r7gPcfcD222e8+klERGoon4lgGen96nel9vpCFxGRHOXzfwTjgBFm9ijRfU3XuPvyPMZTP5SVQVlphcfmzMMeH18GOLiDx14ThjO+Jn3erO8Lz3G59FybcZ5cllODeTLOV1vz1GZMNZk309tzXH99mLdW9kES81a2XTmPrNv9tftR0GXvzNO2QmKJwMweIbqhd0czWwpcS3SbPdz9/4juGXAMUZ/aXxL1z97wuMP61bD2Y/jiY1i3AjZ8Dus/T3/esBY2fQmlG6B0PWxaHz2XboDSr7aM90z3wxCRwpPhpnltdmxYicDds94EO9x79ydJrb/WrVsJH74JK96FVQvg0/fg00XRyX/zhszvsSbQYlso3jZ6btYKmraAbbaPnpsWxx5huKg5NCmCJk1jj0zDsXEWhjGw8Ch/3STza8Jw+WuqmDc17msbWWEw0x0fM4zLOF9V8+S6nBrEVOO4axjTVq+3JvPmuv6kYtjKfZDUvHW6D7ItI38aXBcTdWbDWnjvJZj3HHzwCny2aMu0lttBh29B9/1g252gdactj22233Lib75NvTzoIiJxSgQVfTIHXv0jzHwCNq2D4nawy2AYMCwqku3QE1q1z3eUIiK1Rokg5avP4IVr4M2HoKgF7HUS9BkK3faDIu0mEWm8dIYDWP42PHoGrP0Q9v0xHPT/9KtfRAqGEsHyt+DB46B5axj2HHQdkO+IRETqVGEngnWr4G+nQYs2cPa/Ybud8x2RiEidK+xE8OzV8OVKOPcFJQERKVgNoq+hRCybDu88AQdcAp375jsaEZG8KdxEMOXO6NLQAy/LdyQiInlVmIlg3SqY++/o8tAWbfIdjYhIXhVmIpj1D9i8Efqdme9IRETyrjATwYIXYbsesGOvfEciIpJ3hZcINpfCov/BNw/LdyQiIvVC4SWCle/Cxi+g+/75jkREpF4ovESw/K3oeac++Y1DRKSeKMxE0KxV1I20iIgUYCJYtQA67hpu5CIiIoWXCD77ANqpOwkRkZTCSgRlZbB6sfoVEhGJKaxEkLq/sEoEIiLlCisRrF4cPSsRiIiUK6xEsG5F9Nx6h/zGISJSjxRWIvhyVfTcqkN+4xARqUeUCEREClxBJYJ1qz/Bm7aE5q3yHYqISL1RMIlg3sdreXbaLNYVtc13KCIi9UrBJILFq76kHWtZ5a3zHYqISL1SMDevb9IE2tla1jbZNt+hiIjUKwVTIjAz2vEFXygRiIikKZhE0MSMlraRDdYi36GIiNQrBZQIoCUbWI8SgYhIXMEkgiIzitnIRiUCEZE0BZMIDKfYNrHemuc7FBGReqVgEkHTsg0AqhoSEamggBLBegA2qEQgIpKmYBJB0eavAJj36eY8RyIiUr8UTiIo2wjABm+W50hEROqXgkkETXAAymjC5jLPczQiIvVHoonAzI4ys3fNbIGZjcwwvbuZvWxmb5rZ22Z2TFKxFMVebywtS2o1IiINTmKJwMyKgLuBo4GewFAz61lhtl8Cj7t7P+A04A/JxRM9O8aGUrUTiIikJFkiGAgscPeF7r4ReBQ4vsI8DqQ6/2kLfJhUMGZbqoMufXRGUqsREWlwkkwEXYAlseGlYVzcKOBMM1sKPANcnGlBZjbczErMrGTFihU1DCcqEjjGpHk1XYaISOOT726ohwJj3f02M9sf+KuZ9XL3tEp8d78XuBdgwIABNWrpNU9/2y+fmsllR+xGsyYhF1pUfWQ1WbiISB1o0bSI5k1r//d7kolgGdAtNtw1jIs7FzgKwN2nmlkx0BH4pLaD8XDVUCodPPTqYh56dXFtr0ZEJDE3ntCLM/fbudaXm2QieB3Y1cx2IUoApwGnV5hnMXA4MNbM9gSKgTqpt9ljxzZ8r09niptF1xO565JSEanf+nffLpHlJpYI3L3UzEYAzxFdvXm/u88ys+uBEncfB1wJ/MnMLif6sX62J3RGTi3VMYYO7MZvv987idWIiDQ4ibYRuPszRI3A8XHXxF7PBgYlGUNszeWvLj18t7pZpYhIA1Aw/yxOcYwd2xbnOwwRkXqjcBKB2gBERDIqoESQ9iQiIkHhJAKlABGRjAomEXh5ItBfxkRE4qq8asjMtgfOB3rE53f3YcmFlQSVCEREMsnl8tF/Af8DJgDqtlNEpJHJJRG0cvefJh5JwlyNxSIiGeXSRjA+yRvG1JWKnc6JiEgkl0RwKVEyWG9ma8Pj86QDq22pNHD+Qd/MaxwiIvVNlVVD7t6mLgJJXpQKipsWVTGfiEhhyamvITM7DjgoDE509/HJhZSM8jYC0+WjIiJxVVYNmdloouqh2eFxqZn9NunAap/aCEREMsmlRHAM0Dd11zAzexB4E/hZkoHVtvI0oAKBiEiaXP9Z3C72um0SgSTNQipQHhARSZdLieC3wJtm9jLRefQgYGSiUSXAVSQQEckol6uGHjGzicA+YdRP3f2jRKNKQioTqLFYRCRNpVVDZrZHeO4P7AQsDY/OYVwDpUQgIhKXrURwBTAcuC3DNAcOSySixJTlOwARkXqp0kTg7sPDy6PdfX18mpk14Hs9qkQgIhKXy1VDU3IcV7+pryERkYwqLRGY2Y5AF6ClmfVjy0/pbYFWdRBbIkyNxSIiabK1EXwHOBvoStROkDqDfg78PNmwap8KBCIimWVrI3gQeNDMTnL3J+swpmSpRCAikiaXNoK9zaz8n8Vmtp2Z3ZhgTMlQkUBEJKNcEsHR7r46NeDunxH1P9QgqY1ARCRdLomgyMxapAbMrCXQIsv89VLoM09ERCrIpa+hh4EXzeyBMHwO8GByISVNJQIRkbhc+hq6yczeBg4Po25w9+eSDav2ue5HICKSUU53KHP3Z4FnE46lbqiNQEQkTS53KPu+mc03szVm9nlDvXm96aohEZGMcikR3Ax8z93nJB1MkrbcjkAlAhGRuFyuGvq4oScBoPx/BEoDIiLpcikRlJjZY8BTwIbUSHf/R2JRJcDLn5UKRETickkE2wJfAkNi4xxoUImg/J7FygMiImlyuXz0nLoIJHG6Z7GISEZVJoLwR7KvXXLj7sMSiSgxaiMQEckkl6qh8bHXxcCJwIfJhJMcXTUkIpJZLlVDaV1Qm9kjwORcFm5mRwG/B4qAP7v76Azz/AAYRXSufsvdT89l2dVW/j8CJQIRkbic/llcwa7ADlXNZGZFwN3AkcBS4HUzG+fus2Pz7Ar8DBjk7p+ZWZXL3WoqEYiIpMmljWAt6W0EHwE/zWHZA4EF7r4wLOdR4Hhgdmye84G7Q9fWuPsnOcZdffofgYhIRtnuWTzI3V8Btnf39TVYdhdgSWx4KbBvhXl2C+t6haj6aJS7/ydDLMOB4QDdu3evQSjxhW3d20VEGpts/yweE56nJLj+pkRVTYcAQ4E/xe+GluLu97r7AHcfsP3229doRep9VEQks2xVQ5vM7F6gq5mNqTjR3S+pYtnLgG6x4a5hXNxS4DV33wS8b2bziBLD61VGXmMqEoiIxGUrERwLvAR8BUzP8KjK68CuZraLmTUHTgPGVZjnKaLSAGbWkaiqaGE14s+deh8VEcmo0hKBu68EHjWzOe7+VnUX7O6lZjYCeI6o/v9+d59lZtcDJe4+LkwbYmazgc3A/3P3VTXakhzpoiERkXS5/I+g2kkg9t5ngGcqjLsm9tqBK8IjUVvKA8oEIiJxuXRD3UjoD2UiIpkUTiIIbQSuuiERkTTZ/keQtbrG3W+v/XASpD+UiYhklK2NoE143h3Yhy1X/HwPmJZkUElQp3MiIpllu2roOgAz+y/Q393XhuFRwL/rJLpapRKBiEgmubQRdAI2xoY3hnENlFKBiEhcLr2P/gWYZmb/JDqLHg+MTTKoJJj+TyYiklEu/yP4tZk9Cwwmql85x93fTDyypKiNQEQkTa73I9gMlBElgrLkwkmON8ywRUQSV2UbgZldCjwMdCS6Ic1DZnZx0oElxVQiEBFJk0uJ4FxgX3dfB2BmNwFTgTuTDKzWqdM5EZGMcrlqyIiqhlI205AvvVGJQEQkTS4lggeA1ypcNXRfolElwFUiEBHJKJerhm43s4nAgTSCq4asgLpXEhHJRa5nxc1ESaDBXjWkNgIRkcwK76qhJmojEBGJK5yrhkREJKOCuWoo1VisCiIRkXTVvWoI4AQa4FVDphQgIpJRrlcNTQIGhVEN8qoh3bNYRCSzXPsamgEsT81vZt3dfXFiUSUi3I9AeUBEJE2ViSBcIXQt8DFb2gcc6J1saLWr/OpRZQIRkTS5lAguBXZ391VJB5MkK79DmRKBiEhcLlcNLQHWJB1I3VEiEBGJq7REYGZXhJcLgYlm9m9gQ2q6u9+ecGy1K1U3pDwgIpImW9VQm/C8ODyah0eDtKWHCWUCEZG4ShOBu19Xl4EkzXTVkIhIRtmqhn7n7peZ2dNk+EOuux+XaGSJUSYQEYnLVjX01/B8a10EkjTXP4tFRDLKVjU0PTxPqrtwkqd7FouIpMtWNTSTzH20GeDu3qD+UKb7EYiIZJataujYOouiLqlEICKSJlvV0Aep12a2M7Cru08ws5bZ3ld/qUQgIpJJLncoOx94ArgnjOoKPJVkUElSFxMiIuly6WLiJ0RdUH8O4O7ziW5Z2aC42ghERDLKJRFscPeNqQEza0oDrmfRPYtFRNLlkggmmdnPgZZmdiTwd+DpZMNKgEoEIiIZ5ZIIRgIrgJnABcAz7v6LRKNKkOe0ySIihSOXs+Iod/+Tu5/i7icD95vZw7ks3MyOMrN3zWyBmY3MMt9JZuZmNiDXwEVEpHbkkgi6mdnPAMysOfAkML+qN5lZEXA3cDTQExhqZj0zzNeG6OY3r1Uj7mpTY7GISGa5JIJhwF4hGYwHJrn7qBzeNxBY4O4LQ2Pzo8DxGea7AbgJWJ9byFtHXUyIiKSrNBGYWX8z6w/0A34PnEpUEpgUxlelC9HdzVKWhnFp6wC6ufu/qxt4dVnDvdBJRCRR2f4hfFuF4c+IqnhuI7p89LCtWbGZNQFuB87OYd7hwHCA7t27b81qVSIQEakgWxcTh27lspcB3WLDXcO4lDZAL6LbYALsCIwzs+PcvaRCLPcC9wIMGDCgRj/t1UYgIpJZtt5Hz3T3h2L3Lk6Twz2LXwd2NbNdiBLAacDpsfevATrG1jcRuKpiEqhtKhGIiKTLVjW0TXhuk2FalT+v3b3UzEYAzwFFwP3uPsvMrgdK3H1ctaPdKmV1uzoRkQYiW9XQPeH5a/cuNrPLclm4uz8DPFNh3DWVzHtILsvceioRiIjE1fRvthmri+o1NRGIiGRU00TQYH9Wq9M5EZF0NU0EDe/3ta4aEhHJKNtVQ2up/J7FLROLKCFbNkQlAhGRuGyNxZmuFmqwLJQIdIcyEZF0BdMnc3mJQP8jEBFJUzCJoCE2a4iI1IXCSwQqEYiIpCmcRKACgYhIRgWTCHTVkIhIZgWTCFL3I1DNkIhIuoJJBOVFAmUCEZE0hZMIUiWCPEchIlLfFFwiUIlARCRdASWCiNKAiEi6gksESgUiIukKJxGo91ERkYwKJxGk6H4EIiJpCiYRuP5aLCKSUcEkghQrvE0WEcmqcM6KaiMQEcmocBJBYPofgYhImsJJBCoRiIhkVDiJIFCJQEQkXeEkApUIREQyKpxEkKISgYhImgJKBCoRiIhk0jTfAdSV8jSgEoHUU5s2bWLp0qWsX78+36FIA1ZcXEzXrl1p1qxZzu8pmERgKhFIPbd06VLatGlDjx49dFGD1Ii7s2rVKpYuXcouu+yS8/sKp2poS5Egn1GIVGr9+vV06NBBSUBqzMzo0KFDtUuVhZMIAn3HpD5TEpCtVZPPUAElAlUNiYhkUkCJIGKqGhLJ6PLLL+d3v/td+fB3vvMdzjvvvPLhK6+8kttvv51x48YxevRoAJ566ilmz55dPs8hhxxCSUlJ3QVdwdixYxkxYkS13lNSUsIll1xS7XUtWrSIv/3tb1u9nPqgYBKB6w9lIlkNGjSIKVOmAFBWVsbKlSuZNWtW+fQpU6ZwwAEHcNxxxzFy5Ejg64mgukpLS7cu6K1UWlrKgAEDGDNmTLXfWzER1HQ59UHBXDWUYk0KJvdJA3bd07OY/eHntbrMnp235drvfbvS6QcccACXX345ALNmzaJXr14sX76czz77jFatWjFnzhz69+/P2LFjKSkp4fTTT2fcuHFMmjSJG2+8kSeffBKAv//971x00UWsXr2a++67j8GDB6etZ+LEifzqV79iu+22Y+7cucyZM4eRI0cyceJENmzYwE9+8hMuuOACysrKGDFiBC+99BLdunWjWbNmDBs2jJNPPpkePXpQUlJCx44dKSkp4aqrrmLixIlp63n66ae58cYb2bhxIx06dODhhx+mU6dOjBo1ivfee4+FCxfSvXt3LrjgAm699VbGjx/PMcccw4cffgjA+++/z5gxYzj44IP54Q9/yLp16wC46667OOCAAxg5ciRz5syhb9++nHXWWfTr1698OZ9++inDhg1j4cKFtGrVinvvvZfevXszatQoFi9ezMKFC1m8eDGXXXZZvShFFE4iUIlAJKvOnTvTtGlTFi9ezJQpU9h///1ZtmwZU6dOpW3btuy11140b968fP5U6eDYY4/l5JNPLh9fWlrKtGnTeOaZZ7juuuuYMGHC19b1xhtv8M4777DLLrtw77330rZtW15//XU2bNjAoEGDGDJkCNOnT2fRokXMnj2bTz75hD333JNhw4blvD0HHnggr776KmbGn//8Z26++WZuu+02AGbPns3kyZNp2bJlWgJ55plnAJg+fTrnnHMOJ5xwAs2aNeOFF16guLiY+fPnM3ToUEpKShg9enT5iR9IW861115Lv379eOqpp3jppZf40Y9+xIwZMwCYO3cuL7/8MmvXrmX33XfnwgsvrNY1/0konEQQqI1AGoJsv9yTdMABB/8L9t4AABI1SURBVDBlyhSmTJnCFVdcwbJly5gyZQpt27Zl0KBBOS3j+9//PgB77703ixYtyjjPwIEDy69zf/7553n77bd54oknAFizZg3z589n8uTJnHLKKTRp0oQdd9yRQw89tFrbsnTpUk499VSWL1/Oxo0b066rP+6442jZsmXG961cuZIf/vCHPP7447Rt25Y1a9YwYsQIZsyYQVFREfPmzaty3ZMnTy4vIR122GGsWrWKzz+PSnjf/e53adGiBS1atGCHHXbg448/pmvXrtXattpWQPUkKhGIVCXVTjBz5kx69erFfvvtx9SpU8vbB3LRokULAIqKiiptA9hmm23KX7s7d955JzNmzGDGjBm8//77DBkyJOs6mjZtSllZGUCl18xffPHFjBgxgpkzZ3LPPfekzRdff9zmzZs57bTTuOaaa+jVqxcAd9xxB506deKtt96ipKSEjRs3Zo2tKqn9A9n3UV0qoEQQ6DptkUodcMABjB8/nvbt21NUVET79u1ZvXo1U6dOzZgI2rRpw9q1a7dqnd/5znf44x//yKZNmwCYN28e69atY9CgQTz55JOUlZXx8ccfp1W99OjRg+nTpwOU//KuaM2aNXTp0gWABx98MKdYRo4cSe/evTnttNPSlrPTTjvRpEkT/vrXv7J582Yg+7YPHjyYhx9+GIiqjDp27Mi2226bUwz5kGgiMLOjzOxdM1tgZiMzTL/CzGab2dtm9qKZ7ZxcNB7WmdwaRBq6vfbai5UrV7LffvuljWvbti0dO3b82vynnXYat9xyC/369eO9996r0TrPO+88evbsSf/+/enVqxcXXHABpaWlnHTSSXTt2pWePXty5pln0r9/f9q2bQtEdfCXXnopAwYMoKioKONyR40axSmnnMLee++dMfZMbr31Vp5//nn69u1L3759GTduHBdddBEPPvggffr0Ye7cueWlid69e1NUVESfPn244447vrbu6dOn07t3b0aOHJlzIsoXS+qySjMrAuYBRwJLgdeBoe4+OzbPocBr7v6lmV0IHOLup2Zb7oABA7wm1ym/+rdfs9+8m1lz8TzaduhU7feLJG3OnDnsueee+Q6jXvniiy9o3bo1q1atYuDAgbzyyivsuOOO+Q6r3sv0WTKz6e4+INP8STYWDwQWuPvCEMSjwPFAeSJw95dj878KnJlcOKFEoMZikQbj2GOPZfXq1WzcuJFf/epXSgIJSTIRdAGWxIaXAvtmmf9c4NlME8xsODAcoHv37lsVlKtuSKTBqPjfAElGvWgsNrMzgQHALZmmu/u97j7A3Qdsv/32NVxLqAJTHhARSZNkiWAZ0C023DWMS2NmRwC/AA529w1JBePqhlpEJKMkSwSvA7ua2S5m1hw4DRgXn8HM+gH3AMe5+ycJxlJ+YxrVDImIpEssEbh7KTACeA6YAzzu7rPM7HozOy7MdgvQGvi7mc0ws3GVLK4WKROIiMQl2kbg7s+4+27u/k13/3UYd427jwuvj3D3Tu7eNzyOy77ErYoGUBoQqUx964b6N7/5Ta0spzoWLVpU/o/i6sj1X9cVVdzGmi5na9WLxuI6pbohkYzy0Q11NrkmgtQ/ffMh1T1Ear9VV8VtrOlytlbhdDqn3kelIXl2JHw0s3aXueNecPToSicn2Q31+vXrufDCCykpKaFp06bcfvvtHHrooeXLuuuuu4DofwNXXXUV//nPf/jqq6/o27cv3/72t8u7a0hp3bo1F1xwARMmTODuu+9m0aJFjBkzho0bN7Lvvvvyhz/8gaKiIu677z5uuukm2rVrR58+fWjRogV33XUXZ599dlqvqa1bt+aLL75IW8eiRYsydj9dsRvtefPmlb//mmuuYdy4qIZ7xYoVDBkyhAceeIATTjiBJUuWsH79ei699FKGDx/OyJEjv7aNqeW4O1dffTXPPvssZsYvf/lLTj31VCZOnMioUaPo2LEj77zzDnvvvTcPPfTQVt/itHASQaB7wopklmQ31HfffTdmxsyZM5k7dy5DhgzJ2ovn6NGjueuuu8q7bq5o3bp17Lvvvtx2223MmTOHm266iVdeeYVmzZpx0UUX8fDDD3PEEUdwww038MYbb9CmTRsOO+ww+vTpk/P+2GGHHTJ2Pw3p3WjHXX/99Vx//fWsXr2awYMHl98t7f7776d9+/Z89dVX7LPPPpx00klZt/Ef//gHM2bM4K233mLlypXss88+HHTQQQC8+eabzJo1i86dOzNo0CBeeeUVDjzwwJy3K5MCSgQqEUgDkuWXe5KS6oZ68uTJXHzxxQDsscce7Lzzzjl151yZoqIiTjrpJABefPFFpk+fzj777APAV199xQ477MC0adM4+OCDad++PQCnnHJKtda5adOmSrufjnejXZG7c+aZZ3LFFVew9957AzBmzBj++c9/ArBkyRLmz59Phw4dKl335MmTGTp0KEVFRXTq1ImDDz6Y119/nW233ZaBAweWd1vdt29fFi1apERQXSoRiFSuYjfU3bp147bbbmPbbbflnHPOyWkZuXRDnRLvThoq71K6ouLi4vLO5tyds846i9/+9rdp8zz11FM5rbesrCxj19Lx7qfLysooLi4un1ZZN9YQdTjXtWvX8v01ceJEJkyYwNSpU2nVqhWHHHJIztuZSRLdWBdOY7HaCESqlFQ31PFumefNm8fixYvZfffd6dGjBzNmzKCsrIwlS5Ywbdq08vc0a9asvGvqbA4//HCeeOIJPvkk+ivSp59+ygcffMA+++zDpEmT+OyzzygtLU3rrjrejfW4ceMyrqey7qezefrpp5kwYULavYvXrFnDdtttR6tWrZg7dy6vvvpqlds4ePBgHnvsMTZv3syKFSv473//y8CBA6tcf00VTiIIVCIQqVxS3VBfdNFFlJWVsddee3HqqacyduxYWrRowaBBg9hll13o2bMnl1xyCf379y9/z/Dhw+nduzdnnHFG1ph79uzJjTfeyJAhQ+jduzdHHnkky5cvp0uXLvz85z9n4MCBDBo0iB49epR3Y33++eczadIk+vTpw9SpUzP+wq+s++lsbr/9dpYtW8bAgQPp27cv11xzDUcddRSlpaXsueeejBw5Mm3fVraNJ554Ir1796ZPnz4cdthh3HzzzYl2uJdYN9RJqWk31DNe+Btlbz1Kz588QnHLqg+oSF1TN9S1L9WNdWlpKSeeeCLDhg3jxBNPzHdYiatP3VDXK32PPB2OPD3fYYhIHRo1ahQTJkxg/fr1DBkyhBNOOCHfIdVLBZMIRKTw3HrrrfkOoUEouDYCkfqsoVXVSv1Tk8+QEoFIPVFcXMyqVauUDKTG3J1Vq1alXeqaC1UNidQTXbt2ZenSpaxYsSLfoUgDVlxcXP6Hs1wpEYjUE82aNav036oiSVLVkIhIgVMiEBEpcEoEIiIFrsH9s9jMVgAf1PDtHYGVtRhOQ6BtLgza5sKwNdu8s7tvn2lCg0sEW8PMSir7i3VjpW0uDNrmwpDUNqtqSESkwCkRiIgUuEJLBPfmO4A80DYXBm1zYUhkmwuqjUBERL6u0EoEIiJSgRKBiEiBK4hEYGZHmdm7ZrbAzEbmO57aYmbdzOxlM5ttZrPM7NIwvr2ZvWBm88PzdmG8mdmYsB/eNrP+2ddQf5lZkZm9aWbjw/AuZvZa2LbHzKx5GN8iDC8I03vkM+6aMrN2ZvaEmc01szlmtn9jP85mdnn4XL9jZo+YWXFjO85mdr+ZfWJm78TGVfu4mtlZYf75ZnZWdeNo9InAzIqAu4GjgZ7AUDPrmd+oak0pcKW79wT2A34Stm0k8KK77wq8GIYh2ge7hsdw4I91H3KtuRSYExu+CbjD3b8FfAacG8afC3wWxt8R5muIfg/8x933APoQbXujPc5m1gW4BBjg7r2AIuA0Gt9xHgscVWFctY6rmbUHrgX2BQYC16aSR87cvVE/gP2B52LDPwN+lu+4EtrWfwFHAu8CO4VxOwHvhtf3AENj85fP15AeQNfwBTkMGA8Y0b8tm1Y85sBzwP7hddMwn+V7G6q5vW2B9yvG3ZiPM9AFWAK0D8dtPPCdxnicgR7AOzU9rsBQ4J7Y+LT5cnk0+hIBWz5QKUvDuEYlFIX7Aa8Bndx9eZj0EdApvG4s++J3wNVAWRjuAKx299IwHN+u8m0O09eE+RuSXYAVwAOhOuzPZrYNjfg4u/sy4FZgMbCc6LhNp3Ef55TqHtetPt6FkAgaPTNrDTwJXObun8enefQTodFcI2xmxwKfuPv0fMdSh5oC/YE/uns/YB1bqguARnmctwOOJ0qCnYFt+HoVSqNXV8e1EBLBMqBbbLhrGNcomFkzoiTwsLv/I4z+2Mx2CtN3Aj4J4xvDvhgEHGdmi4BHiaqHfg+0M7PUjZbi21W+zWF6W2BVXQZcC5YCS939tTD8BFFiaMzH+QjgfXdf4e6bgH8QHfvGfJxTqntct/p4F0IieB3YNVxt0JyowWlcnmOqFWZmwH3AHHe/PTZpHJC6cuAsoraD1PgfhasP9gPWxIqgDYK7/8zdu7p7D6Jj+ZK7nwG8DJwcZqu4zal9cXKYv0H9cnb3j4AlZrZ7GHU4MJtGfJyJqoT2M7NW4XOe2uZGe5xjqntcnwOGmNl2oSQ1JIzLXb4bSuqoMeYYYB7wHvCLfMdTi9t1IFGx8W1gRngcQ1Q3+iIwH5gAtA/zG9EVVO8BM4muyMj7dmzF9h8CjA+vvwFMAxYAfwdahPHFYXhBmP6NfMddw23tC5SEY/0UsF1jP87AdcBc4B3gr0CLxnacgUeI2kA2EZX8zq3JcQWGhW1fAJxT3TjUxYSISIErhKohERHJQolARKTAKRGIiBQ4JQIRkQKnRCAiUuCUCCTvzMzN7LbY8FVmNqqWlj3WzE6ues6tXs8poVfQlyuM72xmT4TXfc3smFpcZzszuyjTukSqQ4lA6oMNwPfNrGO+A4mL/YM1F+cC57v7ofGR7v6hu6cSUV+i/3nUVgztgPJEUGFdIjlTIpD6oJToXqyXV5xQ8Re9mX0Rng8xs0lm9i8zW2hmo83sDDObZmYzzeybscUcYWYlZjYv9FWUup/BLWb2eujb/YLYcv9nZuOI/slaMZ6hYfnvmNlNYdw1RH/uu8/Mbqkwf48wb3PgeuBUM5thZqea2TahP/ppoTO548N7zjazcWb2EvCimbU2sxfN7I2w7uPD4kcD3wzLuyW1rrCMYjN7IMz/ppkdGlv2P8zsPxb1XX9zbH+MDbHONLOvHQtpvKrzi0ckSXcDb6dOTDnqA+wJfAosBP7s7gMtukHPxcBlYb4eRP20fxN42cy+BfyI6C/6+5hZC+AVM3s+zN8f6OXu78dXZmadifq535uoL/znzewEd7/ezA4DrnL3kkyBuvvGkDAGuPuIsLzfEHWFMMzM2gHTzGxCLIbe7v5pKBWc6O6fh1LTqyFRjQxx9g3L6xFb5U+i1fpeZrZHiHW3MK0vUU+1G4B3zexOYAegi0d9/xPikQKhEoHUCx71mvoXopuR5Op1d1/u7huI/nafOpHPJDr5pzzu7mXuPp8oYexB1B/Lj8xsBlHX3R2IbvgBMK1iEgj2ASZ61BFaKfAwcFA14q1oCDAyxDCRqJuE7mHaC+7+aXhtwG/M7G2iLge6sKVr4socCDwE4O5zgQ+AVCJ40d3XuPt6olLPzkT75RtmdqeZHQV8nmGZ0kipRCD1ye+AN4AHYuNKCT9YzKwJ0Dw2bUPsdVlsuIz0z3bFflSc6OR6sbundc5lZocQdfNcFww4yd3frRDDvhViOAPYHtjb3TdZ1PNq8VasN77fNhPd6OUzM+tDdPOXHwM/IOq/RgqASgRSb4RfwI+z5faDAIuIqmIAjgOa1WDRp5hZk9Bu8A2iOzs9B1xoUTfemNluFt3sJZtpwMFm1tGiW6AOBSZVI461QJvY8HPAxWZmIYZ+lbyvLdE9GDaFuv6dK1le3P+IEgihSqg70XZnFKqcmrj7k8AviaqmpEAoEUh9cxsQv3roT0Qn37eIbk1Yk1/ri4lO4s8CPw5VIn8mqhZ5IzSw3kMVJWSPuvwdSdQV8lvAdHf/V7b3VPAy0DPVWAzcQJTY3jazWWE4k4eBAWY2k6htY26IZxVR28Y7FRupgT8ATcJ7HgPODlVolekCTAzVVA8R3dJVCoR6HxURKXAqEYiIFDglAhGRAqdEICJS4JQIREQKnBKBiEiBUyIQESlwSgQiIgXu/wONmqkxnCwGhwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUW2zzIjYVvX",
        "colab_type": "text"
      },
      "source": [
        "The likelihood function converges slightly faster for the method without regularization than the method that uses it, but this was likely lucky - it turns out that the $\\theta$ coefficients are relatively close to the \"ones\" matrix used initially. Using a matrix full of 100s yields different results, where the regularization method converges after the method without. The method with regularization converges to a lower likelihood of 0.8, whereas the method without regularization converges to approximately 1. Regularization avoids overfitting of the model, which could be why it converges to a lower likelihood - because it penalizes large parameters, the model performs better on data in the real world, but that means it may sacrifice some accuracy on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAqXTgM5lt56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}