{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_assessment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO8PPR9CihL1Db7daycwpVa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolframalexa/FrequentistML/blob/master/model_assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiXQPh9VywyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assignment\n",
        "\n",
        "# Re-implement the example in section 7.10.2 using any simple, out of the box classifier (like K nearest neighbors from sci-kit). \n",
        "# Reproduce the results for the incorrect and correct way of doing cross-validation."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQyOytqX0Bpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_validate # uses Kfold to cross validate\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_selection import SelectKBest, chi2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdT0vxke0FnK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "e647ce19-605d-447e-868e-525e7a8b02bb"
      },
      "source": [
        "#----------DATA READING \n",
        "data = 'https://raw.githubusercontent.com/wolframalexa/FrequentistML/master/iris.csv' # this is a very popular dataset and does not need cleaning\n",
        "dataframe = pd.read_csv(data, sep=',', header='infer')\n",
        "print(\"Species in dataset:\",dataframe['species'].unique())\n",
        "\n",
        "# split data into inputs and outputs\n",
        "X = dataframe.drop('species', axis=1)\n",
        "y = dataframe['species']\n",
        "y = [0 if x == 'Iris-setosa' else 1 if x == 'Iris-versicolor' else 2 for x in y] # encode species data as integers\n",
        "\n",
        "dataframe.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Species in dataset: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>150.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>150.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.843333</td>\n",
              "      <td>3.054000</td>\n",
              "      <td>3.758667</td>\n",
              "      <td>1.198667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.828066</td>\n",
              "      <td>0.433594</td>\n",
              "      <td>1.764420</td>\n",
              "      <td>0.763161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.300000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.100000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>0.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5.800000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>4.350000</td>\n",
              "      <td>1.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.400000</td>\n",
              "      <td>3.300000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>1.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>7.900000</td>\n",
              "      <td>4.400000</td>\n",
              "      <td>6.900000</td>\n",
              "      <td>2.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sepal_length  sepal_width  petal_length  petal_width\n",
              "count    150.000000   150.000000    150.000000   150.000000\n",
              "mean       5.843333     3.054000      3.758667     1.198667\n",
              "std        0.828066     0.433594      1.764420     0.763161\n",
              "min        4.300000     2.000000      1.000000     0.100000\n",
              "25%        5.100000     2.800000      1.600000     0.300000\n",
              "50%        5.800000     3.000000      4.350000     1.300000\n",
              "75%        6.400000     3.300000      5.100000     1.800000\n",
              "max        7.900000     4.400000      6.900000     2.500000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_c9i51n9C-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "1fde8624-b960-4159-da0e-427183b5b895"
      },
      "source": [
        "#----------THE WRONG WAY\n",
        "\n",
        "# prescreen features by computing their correlation with output class\n",
        "X_new = SelectKBest(chi2, k=2).fit_transform(X,y)\n",
        "print(\"Original data:\\n\",X.loc[[0]])\n",
        "print('Selected data:',X_new[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original data:\n",
            "    sepal_length  sepal_width  petal_length  petal_width\n",
            "0           5.1          3.5           1.4          0.2\n",
            "Selected data: [1.4 0.2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djDiICFyTsl5",
        "colab_type": "text"
      },
      "source": [
        "From inspection, we can see that petal length and petal width are the most important features in the classification. We will build the K nearest neighbours classifier with just *petal_length* and *petal_width*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU4dR7EL9GkL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffef8584-5d61-450d-acaf-906b42905c37"
      },
      "source": [
        "# construct classifier using k nearest neighbours using the two selected features\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X_new, y)\n",
        "\n",
        "# perform cross-validation using kfold\n",
        "cv_results = cross_validate(neigh, X_new,y,cv=2)\n",
        "\n",
        "# compute average error rate\n",
        "avg_error = 1-np.average(cv_results['test_score'])\n",
        "print(\"Average error using wrong method:\",avg_error*100,\"%\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error using wrong method: 4.666666666666663 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JE4lV_BHX78",
        "colab_type": "text"
      },
      "source": [
        "After pre-screening for predictors and choosing the ones having the biggest bearing on the outcome classification, then constructing a model around those traits and performing cross-validation, we obtain on average a 4.67% error rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dn3UiglAebm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-------THE RIGHT WAY\n",
        "\n",
        "# prepare data to work with Kfolds function\n",
        "kf = KFold(n_splits=2, shuffle=True, random_state=0) # draw samples randomly for kfolds\n",
        "\n",
        "X_trains = []\n",
        "X_tests = []\n",
        "y_trains = []\n",
        "y_tests = []\n",
        "X = X.to_numpy()\n",
        "y = np.array(y)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWmC2RFePrdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5498e843-7126-43b4-8daa-961c3f49c306"
      },
      "source": [
        "# split into folds \n",
        "for train_index, test_index in kf.split(X): # there might have been a better way to do this with indexing\n",
        "  X_trains.append(X[train_index])\n",
        "  X_tests.append(X[test_index])\n",
        "  y_trains.append(y[train_index])\n",
        "  y_tests.append(y[test_index])\n",
        "\n",
        "\n",
        "# for each fold, find \"good\" predictors using all samples except fold k\n",
        "errs = 0\n",
        "\n",
        "for fold in range(0,len(X_trains)):\n",
        "  # create arrays dropping the current fold\n",
        "  temp_X_trains = np.delete(X_trains, fold, axis=0)\n",
        "  temp_X_trains = temp_X_trains[0]\n",
        "  temp_X_tests = np.delete(X_tests, fold, axis=0)\n",
        "  temp_y_trains = np.delete(y_trains, fold, axis=0)\n",
        "  temp_y_trains = np.transpose(temp_y_trains)\n",
        "  temp_y_tests = np.delete(y_tests, fold, axis=0)\n",
        "\n",
        "  features = np.array([False, False, False, False])\n",
        "  temp_X_trains_all = np.array([])\n",
        "  temp_y_trains_all = np.array([])\n",
        "\n",
        "  # select good features\n",
        "  kbest = SelectKBest(chi2, k=2)\n",
        "\n",
        "  X_new = kbest.fit_transform(temp_X_trains,temp_y_trains)\n",
        "  features = features | kbest.get_support()             # \"or\" together to find good features\n",
        "\n",
        "  # build a multivariate classifier using all samples except fold k\n",
        "  selected_features = [i for i, feature in enumerate(features) if feature == True]\n",
        "  X_selected = np.concatenate((np.zeros((temp_X_trains.shape[0],len(selected_features))) ,temp_X_trains[:, selected_features]), axis=1) # fit to selected features\n",
        "\n",
        "  neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "  neigh.fit(X_selected, np.ravel(temp_y_trains))\n",
        "\n",
        "# use classifier to predict class labels for samples in fold k\n",
        "  y_predicted = neigh.predict(X_tests[fold])\n",
        "  diff = y_predicted - y_tests[fold]\n",
        "  err = np.count_nonzero(diff)/len(diff)\n",
        "  errs += err\n",
        "\n",
        "# compute average error rate\n",
        "avg_err = errs/len(X_trains)\n",
        "print(\"Average error using correct method:\",avg_err*100,\"%\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average error using correct method: 2.666666666666667 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0vMIWOeb5OV",
        "colab_type": "text"
      },
      "source": [
        "Done correctly, cross-validation should yield a higher average error rate. This is because it has not seen the test samples, so it cannot use them in its inference.\n",
        "\n",
        "However, here the \"correct\" error rate is actually lower than the \"incorrect\" error rate. This is likely because of the size of the dataset and its simplicity - one or two samples can skew the % incorrect by a lot. This is certainly not expected, but it is welcome in terms of getting a model with a lower error."
      ]
    }
  ]
}