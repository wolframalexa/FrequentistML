{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_boosted_trees.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBfDK/kdcEiNQnSnEpcgK2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wolframalexa/FrequentistML/blob/master/gradient_boosted_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liwz4axEHPOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assignment 4: xTreme Gradient Boosted Trees\n",
        "# Select a dataset for either classification or regression. It can be the same dataset \n",
        "# as the one you used in the previous assignments, or a new one. Use an out of the box package \n",
        "# for xTreme gradient boosting trees such as https://xgboost.readthedocs.io/en/latest/.\n",
        "# Use the same 80-10-10 split to tune your classifier/regression method and report your performance and output the feature importance. \n",
        "# Do the features reported make sense? If you are using the same dataset from assignment 1 or 2, do they agree with what you discovered using the Lasso penalty?"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBjIb-CFITyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------IMPORT PACKAGES\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp3gMtF-IeYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------READ DATA\n",
        "\n",
        "data = 'https://raw.githubusercontent.com/wolframalexa/FrequentistML/master/life_expectancy_data.csv'\n",
        "dataframe = pd.read_csv(data, sep=',', header='infer')\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aiwurWVLCGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "427eae9a-3e73-4134-d422-2f51c51609af"
      },
      "source": [
        "#----------CLEAN DATA\n",
        "\n",
        "# Add column to account for intercept \n",
        "ones_col = np.ones(len(dataframe))\n",
        "dataframe.insert(0, \"intercept\", ones_col, True) \n",
        "\n",
        "# Drop rows with missing data\n",
        "clean_data = dataframe.dropna(axis=0)\n",
        "\n",
        "# Rename columns\n",
        "clean_data.columns = ['intercept','country','year','status','life_exp','adult_mort','infant_mort','alcohol','percent_exp','hep_b','measles','bmi','under_five_mort','polio','tot_exp',\n",
        "                      'diphtheria','hiv_aids','gdp','population','thin_1-19','thin_5-9','income_comp_res','schooling'] \n",
        "\n",
        "# Remove rows with invalid data\n",
        "clean_data = clean_data[clean_data['infant_mort'] <= 1000] \n",
        "clean_data = clean_data[clean_data['measles'] <= 1000] \n",
        "clean_data = clean_data[clean_data['under_five_mort'] <= 1000]\n",
        "\n",
        "# Show data information                    \n",
        "clean_data.describe()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intercept</th>\n",
              "      <th>year</th>\n",
              "      <th>life_exp</th>\n",
              "      <th>adult_mort</th>\n",
              "      <th>infant_mort</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>percent_exp</th>\n",
              "      <th>hep_b</th>\n",
              "      <th>measles</th>\n",
              "      <th>bmi</th>\n",
              "      <th>under_five_mort</th>\n",
              "      <th>polio</th>\n",
              "      <th>tot_exp</th>\n",
              "      <th>diphtheria</th>\n",
              "      <th>hiv_aids</th>\n",
              "      <th>gdp</th>\n",
              "      <th>population</th>\n",
              "      <th>thin_1-19</th>\n",
              "      <th>thin_5-9</th>\n",
              "      <th>income_comp_res</th>\n",
              "      <th>schooling</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1357.0</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.00000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1.357000e+03</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "      <td>1357.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2007.941783</td>\n",
              "      <td>70.062122</td>\n",
              "      <td>161.971997</td>\n",
              "      <td>10.862196</td>\n",
              "      <td>4.720442</td>\n",
              "      <td>780.089679</td>\n",
              "      <td>81.265291</td>\n",
              "      <td>91.20339</td>\n",
              "      <td>40.379366</td>\n",
              "      <td>15.097273</td>\n",
              "      <td>85.208548</td>\n",
              "      <td>6.104576</td>\n",
              "      <td>85.543110</td>\n",
              "      <td>1.941489</td>\n",
              "      <td>6123.079003</td>\n",
              "      <td>7.627781e+06</td>\n",
              "      <td>4.215623</td>\n",
              "      <td>4.258364</td>\n",
              "      <td>0.641920</td>\n",
              "      <td>12.409506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>4.041488</td>\n",
              "      <td>8.690290</td>\n",
              "      <td>123.624297</td>\n",
              "      <td>25.177559</td>\n",
              "      <td>4.054261</td>\n",
              "      <td>1883.611933</td>\n",
              "      <td>24.863380</td>\n",
              "      <td>193.88610</td>\n",
              "      <td>19.534123</td>\n",
              "      <td>36.859127</td>\n",
              "      <td>21.512464</td>\n",
              "      <td>2.279412</td>\n",
              "      <td>21.154592</td>\n",
              "      <td>6.180984</td>\n",
              "      <td>12197.905019</td>\n",
              "      <td>2.014408e+07</td>\n",
              "      <td>3.728639</td>\n",
              "      <td>3.802764</td>\n",
              "      <td>0.189328</td>\n",
              "      <td>2.745448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2000.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>5.668726</td>\n",
              "      <td>3.400000e+01</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2005.000000</td>\n",
              "      <td>65.200000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.070000</td>\n",
              "      <td>40.806180</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>22.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>4.600000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>519.292285</td>\n",
              "      <td>1.557560e+05</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>0.542000</td>\n",
              "      <td>10.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2008.000000</td>\n",
              "      <td>72.300000</td>\n",
              "      <td>144.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>4.040000</td>\n",
              "      <td>179.170133</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>46.400000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>5.990000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>1839.729450</td>\n",
              "      <td>1.143896e+06</td>\n",
              "      <td>2.700000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>0.690000</td>\n",
              "      <td>12.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2011.000000</td>\n",
              "      <td>75.400000</td>\n",
              "      <td>219.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>7.440000</td>\n",
              "      <td>579.133164</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>64.00000</td>\n",
              "      <td>56.700000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>7.610000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>5414.634326</td>\n",
              "      <td>5.737723e+06</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>6.500000</td>\n",
              "      <td>0.762000</td>\n",
              "      <td>14.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>723.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>17.870000</td>\n",
              "      <td>18961.348600</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>999.00000</td>\n",
              "      <td>77.100000</td>\n",
              "      <td>879.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>14.390000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>50.600000</td>\n",
              "      <td>119172.741800</td>\n",
              "      <td>1.986867e+08</td>\n",
              "      <td>19.700000</td>\n",
              "      <td>19.900000</td>\n",
              "      <td>0.936000</td>\n",
              "      <td>20.700000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       intercept         year  ...  income_comp_res    schooling\n",
              "count     1357.0  1357.000000  ...      1357.000000  1357.000000\n",
              "mean         1.0  2007.941783  ...         0.641920    12.409506\n",
              "std          0.0     4.041488  ...         0.189328     2.745448\n",
              "min          1.0  2000.000000  ...         0.000000     4.200000\n",
              "25%          1.0  2005.000000  ...         0.542000    10.700000\n",
              "50%          1.0  2008.000000  ...         0.690000    12.500000\n",
              "75%          1.0  2011.000000  ...         0.762000    14.300000\n",
              "max          1.0  2015.000000  ...         0.936000    20.700000\n",
              "\n",
              "[8 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-TxKELcLE2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#----------SEPARATE DATA\n",
        "\n",
        "# Choose seed\n",
        "np.random.seed(500)\n",
        "\n",
        "# Separate training and testing data\n",
        "train_data, validate_data, test_data = np.split(clean_data.sample(frac=1), [int(.8*len(clean_data)), int(.9*len(clean_data))])\n",
        "\n",
        "# Separate training inputs and outputs\n",
        "x_train = train_data.drop(['life_exp','country','status'], axis=1)  \n",
        "y_train = train_data['life_exp']\n",
        "\n",
        "# Separate validation inputs and outputs\n",
        "x_validate = validate_data.drop(['life_exp','country','status'], axis=1)  \n",
        "y_validate = validate_data['life_exp']\n",
        "\n",
        "# Separate testing inputs and outputs\n",
        "x_test = test_data.drop(['life_exp','country','status'], axis=1)\n",
        "y_test = test_data['life_exp']\n",
        "\n",
        "# Normalize testing validation and training inputs\n",
        "x_train = (x_train-x_train.min())/(x_train.max()-x_train.min()) \n",
        "x_train.intercept = np.ones(len(x_train))\n",
        "x_validate = (x_validate-x_validate.min())/(x_validate.max()-x_validate.min()) \n",
        "x_validate.intercept = np.ones(len(x_validate))\n",
        "x_test = (x_test-x_test.min())/(x_test.max()-x_test.min()) \n",
        "x_test.intercept = np.ones(len(x_test))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhP5A3u3aJ9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1651c587-363e-48ee-b729-f0180c4b838a"
      },
      "source": [
        "#--------TRAIN MODEL\n",
        "\n",
        "etas = np.linspace(0,1,10)\n",
        "depth_lim = 5\n",
        "estimators_lim = 5\n",
        "\n",
        "\n",
        "min_score = 100\n",
        "for depth in range(depth_lim): # nest these to get all possible models\n",
        "  for estimators in range(estimators_lim):\n",
        "    for rate in etas:\n",
        "      # make and fit model with these parameters\n",
        "      xgb_model = xgb.XGBRegressor(n_estimators = estimators, learning_rate=rate, objective=\"reg:squarederror\", max_depth=depth)\n",
        "      xgb_model.fit(x_train, y_train)\n",
        "      y_pred = xgb_model.predict(x_validate)\n",
        "\n",
        "      # score model and choose model with lowest error\n",
        "      score = mse(y_validate, y_pred)\n",
        "      if score < min_score:\n",
        "        min_score = score\n",
        "        best_model = xgb_model\n",
        "\n",
        "print(\"The best model is the following:\\n\",best_model)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The best model is the following:\n",
            " XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "             colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
            "             importance_type='gain', learning_rate=0.8888888888888888,\n",
            "             max_delta_step=0, max_depth=4, min_child_weight=1, missing=None,\n",
            "             n_estimators=4, n_jobs=1, nthread=None,\n",
            "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
            "             reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
            "             subsample=1, verbosity=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYIQcaT2dJZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "77b4ec79-1db2-43bb-8e8f-1e04227e30cd"
      },
      "source": [
        "#--------PREDICT AND REPORT FEATURE IMPORTANCE\n",
        "\n",
        "y_pred_test = best_model.predict(x_test)\n",
        "test_score = mse(y_test, y_pred_test)\n",
        "print(\"Score on test set:,\"test_score)\n",
        "print(\"Feature importances:\\n\",list(zip(x_test.columns,xgb_model.feature_importances_)))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.424513591207594\n",
            "Feature importances:\n",
            " [('intercept', 0.0), ('year', 0.0018047928), ('adult_mort', 0.025521992), ('infant_mort', 0.006940946), ('alcohol', 0.022453226), ('percent_exp', 0.010459741), ('hep_b', 0.0), ('measles', 0.00012279741), ('bmi', 0.0075482745), ('under_five_mort', 0.065146804), ('polio', 0.0), ('tot_exp', 0.0077739116), ('diphtheria', 0.0), ('hiv_aids', 0.605227), ('gdp', 0.0), ('population', 0.0), ('thin_1-19', 0.0), ('thin_5-9', 0.07071378), ('income_comp_res', 0.15453555), ('schooling', 0.021751178)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV5-9ONOvHNe",
        "colab_type": "text"
      },
      "source": [
        "The most important features are HIV/AIDS, income, adult mortality, alcohol use, the under-five mortality rate, the thinness rate for children 5-9, and schooling. It makes sense that these would have an impact on life expectancy.\n",
        "\n",
        "This agrees for the most part with the results for project #1 when using Lasso. The intercept and GDP are not important here even though they were included when using the Lasso method, while alcohol usage was included here but not using Lasso. This could be because of correlations between features that results in one being included in Lasso and the other being included when using gradient boosted trees.\n",
        "\n",
        "XGB returns a score on the test set of 7.4, compared to 12.3 using linear regression - the model is more accurate."
      ]
    }
  ]
}